{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to keep things in order, and to avoid to copy and paste everytime our functions if we want to use them in more than one folder,\n",
    "#we can temporarily use this library. \n",
    "import sys\n",
    "\n",
    "#in this way Python will search the implementations also in the path '../HelperFunctions'\n",
    "sys.path.insert(0, '../HelperFunctions')\n",
    "sys.path.insert(0, '../pre-processing/Clean_Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from common_functions import *\n",
    "from counters import *\n",
    "from replace import *\n",
    "from regressors import batch_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(\"../data/train.csv\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will surely be deleted, in this way we are sure that original_data is the original version of the data and we don't have\n",
    "#to load them again\n",
    "from copy import deepcopy\n",
    "originalData = deepcopy(input_data)\n",
    "originalY = deepcopy(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions from lab 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sig(t):\n",
    "#     \"\"\"apply sigmoid function on t.\"\"\"\n",
    "#     if t > 0:\n",
    "#         return 1 / (1 + np.exp(-t))\n",
    "#     else:\n",
    "#         return np.exp(t) / (1 + np.exp(t))\n",
    "\n",
    "# sigmoid = np.vectorize(sig)\n",
    "\n",
    "def sigmoid(t):\n",
    "     return .5*(1+np.tanh(.5 * t))\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    correctZero = 1e-7\n",
    "    loss = np.sum((1 - y) * np.log(1 - pred + correctZero) + y * np.log(pred + correctZero))\n",
    "    #return np.squeeze(- loss)\n",
    "    return -loss/y.shape[0]\n",
    "\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return grad/y.shape[0]\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    #batch = next(batch_iter(y, tx, 32))\n",
    "    #minibatch_y, minibatch_tx = batch[0], batch[1]\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    w = w - gamma * grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "#Creation of tx\n",
    "\n",
    "input_data = deepcopy(originalData)\n",
    "print(input_data.shape)\n",
    "#Clean the dataset\n",
    "numInvalidValues = countInvalid(input_data,-999)\n",
    "idxCols = np.where(numInvalidValues>0)[0]\n",
    "input_data = replaceWithZero(input_data,-999,idxCols)\n",
    "\n",
    "#standardize\n",
    "tx,_,_ = standardize(input_data)\n",
    "\n",
    "#add ones\n",
    "tx = np.c_[np.ones((yb.shape[0], 1)), tx]\n",
    "y = yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 31)\n",
      "Iter = 0, Loss = 0.6931469805599657, |wold - wnew| = 0.0005621292061665366\n",
      "Iter = 100, Loss = 0.6322868630951723, |wold - wnew| = 0.0005413858962153733\n",
      "Iter = 200, Loss = 0.5756629852485209, |wold - wnew| = 0.000523026864314041\n",
      "Iter = 300, Loss = 0.5226572709361063, |wold - wnew| = 0.0005068003548763514\n",
      "Iter = 400, Loss = 0.47275225673990545, |wold - wnew| = 0.0004924296821479786\n",
      "Iter = 500, Loss = 0.42552008039490075, |wold - wnew| = 0.00047965102095496437\n",
      "Iter = 600, Loss = 0.3806083546999344, |wold - wnew| = 0.00046822983664032133\n",
      "Iter = 700, Loss = 0.3377264620444261, |wold - wnew| = 0.00045796506538703645\n",
      "Iter = 800, Loss = 0.2966336651441241, |wold - wnew| = 0.0004486874923474239\n",
      "Iter = 900, Loss = 0.257129326432646, |wold - wnew| = 0.00044025591382528657\n",
      "Iter = 1000, Loss = 0.21904506695911646, |wold - wnew| = 0.00043255283034905167\n",
      "Iter = 1100, Loss = 0.18223855118655638, |wold - wnew| = 0.0004254804258357327\n",
      "Iter = 1200, Loss = 0.14658857614177684, |wold - wnew| = 0.00041895709344942964\n",
      "Iter = 1300, Loss = 0.11199118561920439, |wold - wnew| = 0.00041291453926588527\n",
      "Iter = 1400, Loss = 0.07835658394484869, |wold - wnew| = 0.00040729540436474905\n",
      "Iter = 1500, Loss = 0.04560667325655859, |wold - wnew| = 0.0004020513232460372\n",
      "Iter = 1600, Loss = 0.013673078515920716, |wold - wnew| = 0.0003971413408138164\n",
      "Iter = 1700, Loss = -0.017504444359258846, |wold - wnew| = 0.0003925306217401348\n",
      "Iter = 1800, Loss = -0.047979298084795466, |wold - wnew| = 0.0003881893976016032\n",
      "Iter = 1900, Loss = -0.07779909404943647, |wold - wnew| = 0.0003840921070883933\n",
      "Iter = 2000, Loss = -0.10700651166878632, |wold - wnew| = 0.0003802166928368807\n",
      "Iter = 2100, Loss = -0.13564000526947903, |wold - wnew| = 0.0003765440252734355\n",
      "Iter = 2200, Loss = -0.16373438851000882, |wold - wnew| = 0.0003730574294985102\n",
      "Iter = 2300, Loss = -0.1913213199964614, |wold - wnew| = 0.0003697422958547287\n",
      "Iter = 2400, Loss = -0.2184297088258651, |wold - wnew| = 0.00036658575856934915\n",
      "Iter = 2500, Loss = -0.24508605495442923, |wold - wnew| = 0.00036357642988207195\n",
      "Iter = 2600, Loss = -0.27131473628984915, |wold - wnew| = 0.0003607041794956684\n",
      "Iter = 2700, Loss = -0.2971382520544265, |wold - wnew| = 0.0003579599511357271\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-a0d02800ae71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mwold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-d939dfaaa449>\u001b[0m in \u001b[0;36mlearning_by_gradient_descent\u001b[1;34m(y, tx, w, gamma)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m#minibatch_y, minibatch_tx = batch[0], batch[1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-d939dfaaa449>\u001b[0m in \u001b[0;36mcalculate_loss\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mcorrectZero\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-7\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcorrectZero\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcorrectZero\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;31m#return np.squeeze(- loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#first tries with gradient descent\n",
    "\n",
    "Y, TX = sample_data(y, tx, 10, 150000)\n",
    "max_iter = 10000\n",
    "w = np.zeros(tx.shape[1])\n",
    "gamma = 0.0005\n",
    "loss0 = 0\n",
    "print(TX.shape)\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    wold = w\n",
    "    loss, w = learning_by_gradient_descent(Y, TX, w, gamma)\n",
    "\n",
    "    if iter%100==0:\n",
    "        wnew = w\n",
    "        print(f'Iter = {iter}, Loss = {loss}, |wold - wnew| = {np.linalg.norm(wold-wnew)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to generate a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WWW = w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx, test_data, ids = load_csv_data(\"../data/test.csv\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to preprocess the test_data as we preprocessed the train data\n",
    "\n",
    "numInvalidValues = countInvalid(test_data,-999)\n",
    "idxCols = np.where(numInvalidValues>0)[0]\n",
    "test_data = replaceWithZero(test_data,-999,idxCols)\n",
    "txTest,_,_ = standardize(test_data)\n",
    "txTest = np.c_[np.ones((xxx.shape[0], 1)), txTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(WWW, txTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids, y_pred, 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same things with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../pre-processing/PCA/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "from pca_functions import PCAWithCovariance\n",
    "\n",
    "input_data = deepcopy(originalData)\n",
    "y = deepcopy(originalY)\n",
    "print(input_data.shape)\n",
    "print(y.shape)\n",
    "numInvalidValues = countInvalid(input_data,-999)\n",
    "idxCols = np.where(numInvalidValues>0)[0]\n",
    "input_data = replaceWithZero(input_data,-999,idxCols)\n",
    "input_data,_,_ = standardize(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the principal components\n",
    "\n",
    "_,eV = PCAWithCovariance(input_data)\n",
    "\n",
    "N = 2 #num p. components\n",
    "components = np.empty(input_data.shape[0])\n",
    "for i in range(N):\n",
    "    components = np.c_[components, input_data.dot(eV[:,i])]\n",
    "    \n",
    "#tx = np.c_[np.ones(input_data.shape[0]), components]\n",
    "tx = components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y, TX = sample_data(y, tx, 1, 70000)\n",
    "TX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter = 0, Loss = 0.6931469805599655, |wold - wnew| = 0.0031179742471667757\n",
      "Iter = 500, Loss = 0.5447375044285359, |wold - wnew| = 0.0004555830001966787\n",
      "Iter = 1000, Loss = 0.5342454037057077, |wold - wnew| = 0.0002189034962146976\n",
      "Iter = 1500, Loss = 0.5314790629170908, |wold - wnew| = 0.00012074029204930974\n",
      "Iter = 2000, Loss = 0.5305906522236288, |wold - wnew| = 7.059816906828605e-05\n",
      "Iter = 2500, Loss = 0.5302781788975787, |wold - wnew| = 4.256537382383867e-05\n",
      "Iter = 3000, Loss = 0.5301627597796059, |wold - wnew| = 2.6113614835430183e-05\n",
      "Iter = 3500, Loss = 0.5301189127039768, |wold - wnew| = 1.6185892921585408e-05\n",
      "Iter = 4000, Loss = 0.5301019756231156, |wold - wnew| = 1.0095015865962806e-05\n",
      "Iter = 4500, Loss = 0.5300953673352786, |wold - wnew| = 6.320300985399138e-06\n",
      "Iter = 5000, Loss = 0.5300927735057194, |wold - wnew| = 3.9664210696567164e-06\n",
      "Iter = 5500, Loss = 0.5300917519048081, |wold - wnew| = 2.492888464328479e-06\n",
      "Iter = 6000, Loss = 0.5300913488693026, |wold - wnew| = 1.5682293300647031e-06\n",
      "Iter = 6500, Loss = 0.5300911898213627, |wold - wnew| = 9.871179836585045e-07\n",
      "Iter = 7000, Loss = 0.5300911271221357, |wold - wnew| = 6.215662708802526e-07\n",
      "Iter = 7500, Loss = 0.5300911024694448, |wold - wnew| = 3.9147656869002184e-07\n",
      "Iter = 8000, Loss = 0.5300910928227791, |wold - wnew| = 2.4659656783073327e-07\n",
      "Iter = 8500, Loss = 0.5300910890790052, |wold - wnew| = 1.5534880026104355e-07\n",
      "Iter = 9000, Loss = 0.5300910876462364, |wold - wnew| = 9.787093423795178e-08\n",
      "Iter = 9500, Loss = 0.5300910871109759, |wold - wnew| = 6.16616702603326e-08\n",
      "Iter = 10000, Loss = 0.5300910869195725, |wold - wnew| = 3.884961664879892e-08\n",
      "Iter = 10500, Loss = 0.5300910868568651, |wold - wnew| = 2.4477351756252483e-08\n",
      "Iter = 11000, Loss = 0.5300910868403342, |wold - wnew| = 1.5422190069278122e-08\n",
      "Iter = 11500, Loss = 0.5300910868390404, |wold - wnew| = 9.716954140491896e-09\n",
      "Iter = 12000, Loss = 0.5300910868418466, |wold - wnew| = 6.1223170136562054e-09\n",
      "Iter = 12500, Loss = 0.5300910868450519, |wold - wnew| = 3.8574690380256805e-09\n",
      "Iter = 13000, Loss = 0.5300910868476422, |wold - wnew| = 2.4304669159656637e-09\n",
      "Iter = 13500, Loss = 0.5300910868495008, |wold - wnew| = 1.5313602294298577e-09\n",
      "Iter = 14000, Loss = 0.5300910868507618, |wold - wnew| = 9.648621238854285e-10\n",
      "Iter = 14500, Loss = 0.530091086851592, |wold - wnew| = 6.079295874287734e-10\n",
      "Iter = 15000, Loss = 0.5300910868521294, |wold - wnew| = 3.8303765145650363e-10\n",
      "Iter = 15500, Loss = 0.5300910868524735, |wold - wnew| = 2.4134026056379786e-10\n",
      "Iter = 16000, Loss = 0.5300910868526925, |wold - wnew| = 1.5206105751298205e-10\n",
      "Iter = 16500, Loss = 0.5300910868528314, |wold - wnew| = 9.580895183852993e-11\n",
      "Iter = 17000, Loss = 0.5300910868529193, |wold - wnew| = 6.036625333532499e-11\n",
      "Iter = 17500, Loss = 0.5300910868529748, |wold - wnew| = 3.803492788611756e-11\n",
      "Iter = 18000, Loss = 0.5300910868530099, |wold - wnew| = 2.3964695169257697e-11\n",
      "Iter = 18500, Loss = 0.5300910868530319, |wold - wnew| = 1.5099399042528092e-11\n",
      "Iter = 19000, Loss = 0.5300910868530458, |wold - wnew| = 9.513646192425216e-12\n",
      "Iter = 19500, Loss = 0.5300910868530546, |wold - wnew| = 5.99423667187316e-12\n"
     ]
    }
   ],
   "source": [
    "max_iter = 10000\n",
    "w = np.zeros(TX.shape[1])\n",
    "gamma = 0.005\n",
    "loss0 = 0\n",
    "for iter in range(max_iter):\n",
    "    wold = w\n",
    "    loss, w = learning_by_gradient_descent(Y, TX, w, gamma)\n",
    "\n",
    "    if iter%500==0:\n",
    "        wnew = w\n",
    "        print(f'Iter = {iter}, Loss = {loss}, |wold - wnew| = {np.linalg.norm(wold-wnew)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate submission with PCA\n",
    "xxx, test_data, ids = load_csv_data(\"../data/test.csv\", sub_sample=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numInvalidValues = countInvalid(test_data,-999)\n",
    "idxCols = np.where(numInvalidValues>0)[0]\n",
    "test_data = replaceWithZero(test_data,-999,idxCols)\n",
    "test_data,_,_ = standardize(test_data)\n",
    "\n",
    "components = np.empty(test_data.shape[0])\n",
    "for i in range(N):\n",
    "    components = np.c_[components, test_data.dot(eV[:,i])]\n",
    "    \n",
    "#tx_test = np.c_[np.ones(test_data.shape[0]), components]\n",
    "tx_test = components\n",
    "\n",
    "y_pred = predict_labels(w, tx_test)\n",
    "create_csv_submission(ids, y_pred, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

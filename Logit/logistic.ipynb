{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to keep things in order, and to avoid to copy and paste everytime our functions if we want to use them in more than one folder,\n",
    "#we can temporarily use this library. \n",
    "import sys\n",
    "\n",
    "#in this way Python will search the implementations also in the path '../HelperFunctions'\n",
    "sys.path.insert(0, '../HelperFunctions')\n",
    "sys.path.insert(0, '../pre-processing/Clean_Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from common_functions import *\n",
    "from counters import *\n",
    "from remove import *\n",
    "from replace import *\n",
    "from regressors import batch_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(\"../data/train.csv\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "(array([     1,      2,      3, ..., 249996, 249998, 249999]),)\n",
      "[1. 0. 0. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#this will surely be deleted, in this way we are sure that original_data is the original version of the data and we don't have\n",
    "#to load them again\n",
    "from copy import deepcopy\n",
    "originalData = deepcopy(input_data)\n",
    "originalY = deepcopy(yb)\n",
    "print(yb)\n",
    "\n",
    "idx_wrong=np.where(yb==-1)\n",
    "print(idx_wrong)\n",
    "yb[idx_wrong]=0\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions from lab 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return np.exp(t)/(1 + np.exp(t))\n",
    "\n",
    "\n",
    "#sigmoid = np.vectorize(sig)\n",
    "\n",
    "def sigmoid(t):\n",
    "     return .5*(1+np.tanh(.5 * t))\n",
    "\n",
    "#Is it okay the multiplication between vectors??\n",
    "# Why do you divide by the number of samples? (mean)\n",
    "#def calculate_loss(y, tx, w):\n",
    "#   \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    " #   pred = sigmoid(tx.dot(w))\n",
    " #   correctZero = 1e-7\n",
    "   # loss = np.sum((1 - y) * np.log(1 - pred + correctZero) + y * np.log(pred + correctZero))\n",
    "    #return np.squeeze(- loss)\n",
    "    #return -loss/y.shape[0]\n",
    "\n",
    "# Just to try again the function of the professor --> good idea to add a correction to avoid the log(0)\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    #pred = sig(tx.dot(w))\n",
    "    correctZero = 1e-7\n",
    "    #loss = y.T.dot(np.log(pred+correctZero)) + (1 - y).T.dot(np.log(1 - pred + correctZero))\n",
    "    #loss= y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    loss = np.ones(tx.shape[0]).dot(np.log(1+np.exp(tx.dot(w))))-y.dot(tx.dot(w))\n",
    "    return np.squeeze(-loss)\n",
    "\n",
    "# same question as above\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    pred = sig(tx.dot(w))\n",
    "    print('pred =',pred)\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return grad\n",
    "    #return grad/y.shape[0]\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    #batch = next(batch_iter(y, tx, 32))\n",
    "    #minibatch_y, minibatch_tx = batch[0], batch[1]\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    print('grad =', grad)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    print('loss =', loss)\n",
    "    w = w - gamma * grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(250000, 19)\n",
      "(250000, 20)\n"
     ]
    }
   ],
   "source": [
    "#Creation of tx\n",
    "\n",
    "input_data = deepcopy(originalData)\n",
    "print(input_data.shape)\n",
    "\n",
    "#Clean the dataset\n",
    "#numInvalidValues = countInvalid(input_data,-999)\n",
    "#idxCols = np.where(numInvalidValues>0)[0]\n",
    "#input_data = replaceWithZero(input_data,-999,idxCols)\n",
    "\n",
    "# Stocking the indexes of columns to remove\n",
    "idxCols = np.where(countInvalid(input_data,-999)>0)[0]\n",
    "input_data=removeColumns(input_data,0)\n",
    "print(input_data.shape)\n",
    "\n",
    "#standardize\n",
    "tx,_,_ = standardize(input_data)\n",
    "\n",
    "#add ones\n",
    "tx = np.c_[np.ones((yb.shape[0], 1)), tx]\n",
    "y = yb\n",
    "print(tx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3, 4.7, 8], [0.4, 5, 6], [2, 5, 1.2]]\n",
      "[1.23333333 4.9        5.06666667]\n",
      "[[ 0.06666667 -0.2         2.93333333]\n",
      " [-0.83333333  0.1         0.93333333]\n",
      " [ 0.76666667  0.1        -3.86666667]]\n",
      "std = [0.65489609 0.14142136 2.85345794]\n",
      "[[ 0.10179732 -1.41421356  1.02799249]\n",
      " [-1.2724665   0.70710678  0.32708852]\n",
      " [ 1.17066918  0.70710678 -1.35508101]]\n"
     ]
    }
   ],
   "source": [
    "#To understand better standardization\n",
    "A=[[1.3,4.7,8],[0.4,5,6],[2,5,1.2]]\n",
    "print(A)\n",
    "print(np.mean(A,axis=0))\n",
    "print(A-np.mean(A,axis=0))\n",
    "a=A-np.mean(A,axis=0)\n",
    "b=np.std(A,axis=0)\n",
    "print('std =',b)\n",
    "print(a/np.std(A,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred = [0.86261365 0.5408875  0.94381645 ... 0.23196916 0.12924795 0.29528372]\n",
      "grad = [ 44950.00262175  24621.69674809  29216.41193362 -16095.05306724\n",
      "  32408.05372318  40188.62868644  41381.04043039   3983.82038734\n",
      "  -2799.24031951  18241.54694441   9563.62080475  42236.60148554\n",
      "  18560.65791435   7959.31203243  33193.56432465  11281.25462266\n",
      "  40024.50367812  38477.84207328  38574.03289732]\n",
      "loss = -251852.47360791118\n",
      "w = [-44.65000262 -24.32169675 -28.91641193  16.39505307 -32.10805372\n",
      " -39.88862869 -41.08104043  -3.68382039   3.09924032 -17.94154694\n",
      "  -9.2636208  -41.93660149 -18.26065791  -7.65931203 -32.89356432\n",
      " -10.98125462 -39.72450368 -38.17784207 -38.2740329 ]\n",
      "iter =  0\n",
      "pred = [1.09130120e-060 1.00000000e+000 2.68232031e-165 ... 1.00000000e+000\n",
      " 1.00000000e+000 1.00000000e+000]\n",
      "grad = [ 28703.76683368 -14638.34817753 -95107.6220295   37617.71306059\n",
      " -38488.96562231 -98235.60104415  -9976.24622094 -71519.71233206\n",
      " -52392.51832738 -11902.82100603  -3538.31355604 -50245.0795188\n",
      " -13061.30073052  -2344.93428222 -55549.50028218  -5995.60326017\n",
      " -93029.11276377 -94903.85960893 -92674.78847186]\n",
      "loss = -23476633.105339386\n",
      "w = [-73.35376946  -9.68334857  66.1912101  -21.22265999   6.3809119\n",
      "  58.34697236 -31.10479421  67.83589194  55.49175865  -6.03872594\n",
      "  -5.72530725   8.30847803  -5.19935718  -5.31437775  22.65593596\n",
      "  -4.98565136  53.30460909  56.72601754  54.40075557]\n",
      "iter =  1\n",
      "pred = [1.00000000e+000 9.51689823e-011 4.43144359e-099 ... 1.72433905e-018\n",
      " 6.00974560e-064 3.89856826e-149]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = -inf\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "iter =  2\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "iter =  3\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "iter =  4\n",
      "pred = [nan nan nan ... nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "iter =  5\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "iter =  6\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "iter =  7\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "iter =  8\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "iter =  9\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "pred = [nan nan nan ... nan nan nan]\n",
      "grad = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n",
      "loss = nan\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan]\n"
     ]
    }
   ],
   "source": [
    "#first tries with gradient descent\n",
    "\n",
    "#Y, TX = sample_data(y, tx, 10, 150000)\n",
    "max_iter = 100\n",
    "w = (0.3)*np.ones(tx.shape[1])\n",
    "gamma = 0.001\n",
    "loss0 = 0\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    w_old = w\n",
    "    loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "    print('w =',w)\n",
    "    #gamma=0.55*gamma\n",
    "    if iter<10:\n",
    "        w_new = w\n",
    "        print('iter = ', iter)\n",
    "        #print(f'Iter = {iter}, Loss = {loss}, |w_old - w_new| = {np.linalg.norm(w_old-w_new)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to generate a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WWW = w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx, test_data, ids = load_csv_data(\"../data/test.csv\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to preprocess the test_data as we preprocessed the train data\n",
    "\n",
    "numInvalidValues = countInvalid(test_data,-999)\n",
    "idxCols = np.where(numInvalidValues>0)[0]\n",
    "test_data = replaceWithZero(test_data,-999,idxCols)\n",
    "txTest,_,_ = standardize(test_data)\n",
    "txTest = np.c_[np.ones((xxx.shape[0], 1)), txTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(WWW, txTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids, y_pred, 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same thing with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../pre-processing/PCA/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_functions import PCAWithCovariance\n",
    "\n",
    "input_data = deepcopy(originalData)\n",
    "y = deepcopy(originalY)\n",
    "print(input_data.shape)\n",
    "print(y.shape)\n",
    "numInvalidValues = countInvalid(input_data,-999)\n",
    "idxCols = np.where(numInvalidValues>0)[0]\n",
    "input_data = replaceWithZero(input_data,-999,idxCols)\n",
    "input_data,_,_ = standardize(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_functions import PCAWithCovariance\n",
    "\n",
    "input_data = deepcopy(originalData)\n",
    "y = deepcopy(originalY)\n",
    "print(input_data.shape)\n",
    "print(y.shape)\n",
    "input_data=removeColumns(input_data,0)\n",
    "print(input_data.shape)\n",
    "input_data,_,_ = standardize(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the principal components\n",
    "\n",
    "_,eV = PCAWithCovariance(input_data)\n",
    "\n",
    "N = 7 #num p. components\n",
    "components = np.empty(input_data.shape[0])\n",
    "for i in range(N):\n",
    "    components = np.c_[components, input_data.dot(eV[:,i])]\n",
    "    \n",
    "print(components.shape)\n",
    "print(components[:,0])\n",
    "tx = np.c_[np.ones(input_data.shape[0]), components]\n",
    "print(tx.shape)\n",
    "print(tx[:,0])\n",
    "print(tx[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, TX = sample_data(y, tx, 1, 70000)\n",
    "TX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 1000\n",
    "w = np.ones(tx.shape[1])\n",
    "gamma = 10000\n",
    "loss0 = 0\n",
    "for iter in range(max_iter):\n",
    "    wold = w\n",
    "    loss, w = learning_by_gradient_descent(yb, tx, w, gamma)\n",
    "    #gamma=0.55*gamma\n",
    "    if iter%10==0:\n",
    "        wnew = w\n",
    "        print(f'Iter = {iter}, Loss = {loss}, |wold - wnew| = {np.linalg.norm(wold-wnew)}')\n",
    "print('final loss =',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate submission with PCA\n",
    "xxx, test_data, ids = load_csv_data(\"../data/test.csv\", sub_sample=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numInvalidValues = countInvalid(test_data,-999)\n",
    "idxCols = np.where(numInvalidValues>0)[0]\n",
    "test_data = replaceWithZero(test_data,-999,idxCols)\n",
    "test_data,_,_ = standardize(test_data)\n",
    "\n",
    "components = np.empty(test_data.shape[0])\n",
    "for i in range(N):\n",
    "    components = np.c_[components, test_data.dot(eV[:,i])]\n",
    "    \n",
    "#tx_test = np.c_[np.ones(test_data.shape[0]), components]\n",
    "tx_test = components\n",
    "\n",
    "y_pred = predict_labels(w, tx_test)\n",
    "create_csv_submission(ids, y_pred, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import time \n",
    "\n",
    "#to keep things in order, and to avoid to copy and paste everytime our functions if we want to use them in more than one folder,\n",
    "#we can temporarily use this library. \n",
    "import sys\n",
    "\n",
    "#in this way Python will search the implementations also in the path '../HelperFunctions'\n",
    "sys.path.insert(0, '../HelperFunctions')\n",
    "sys.path.insert(0, '../pre-processing/Clean_Data/')\n",
    "\n",
    "from proj1_helpers import *\n",
    "from common_functions import *\n",
    "from counters import *\n",
    "from remove import *\n",
    "from replace import *\n",
    "from regressors import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "(array([     1,      2,      3, ..., 249996, 249998, 249999]),)\n",
      "[1. 0. 0. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "yb, input_data, ids = load_csv_data(\"../data/train.csv\", sub_sample=False)\n",
    "_, test_data, ids_test = load_csv_data(\"../data/test.csv\", sub_sample=False)\n",
    "\n",
    "#this will surely be deleted, in this way we are sure that original_data is the original version of the data and we don't have\n",
    "#to load them again\n",
    "from copy import deepcopy\n",
    "originalData = deepcopy(input_data)\n",
    "originalY = deepcopy(yb)\n",
    "print(yb)\n",
    "\n",
    "# change the label of -1 to 0 for simplicity. \n",
    "idx_wrong=np.where(yb==-1)\n",
    "print(idx_wrong)\n",
    "yb[idx_wrong]=0\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the columns with bad data and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = deepcopy(originalData)\n",
    "y = deepcopy(originalY)\n",
    "print(input_data.shape)\n",
    "print(y.shape)\n",
    "# Stocking the indexes of columns to remove\n",
    "idxCols = np.where(countInvalid(input_data,-999)>0)[0]\n",
    "input_data=removeColumns(input_data,0)\n",
    "print(input_data.shape)\n",
    "input_data,_,_ = standardize(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same pre-processing applied on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.shape)\n",
    "# To remove the same data we removed from the train set.\n",
    "test_data=np.delete(test_data,idxCols,axis=1)\n",
    "print(test_data.shape)\n",
    "test_data,_,_ = standardize(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 8)\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "(250000, 9)\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Generating the principal components\n",
    "sys.path.insert(0, '../pre-processing/PCA/')\n",
    "from pca_functions import PCAWithCovariance\n",
    "\n",
    "input_data,_,_ = standardize(input_data)\n",
    "test_data,_,_ = standardize(test_data)\n",
    "\n",
    "_,eV = PCAWithCovariance(input_data)\n",
    "\n",
    "N = 7 #num p. components\n",
    "components = np.empty(input_data.shape[0])\n",
    "for i in range(N):\n",
    "    components = np.c_[components, input_data.dot(eV[:,i])]\n",
    "    \n",
    "print(components.shape)\n",
    "print(components[:,0])\n",
    "input_data = np.c_[np.ones(input_data.shape[0]), components]\n",
    "print(input_data.shape)\n",
    "print(input_data[:,0])\n",
    "print(input_data[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA applied on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 7 #num p. components\n",
    "components = np.empty(test_data.shape[0])\n",
    "for i in range(N):\n",
    "    components = np.c_[components, test_data.dot(eV[:,i])]\n",
    "test_data = np.c_[np.ones(test_data.shape[0]), components]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Gradient Descent\n",
    "\n",
    "#### Creation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=0.171334\n",
      "Gradient Descent(10/99): loss=0.15268659633285983\n",
      "Gradient Descent(20/99): loss=0.14084488862166206\n",
      "Gradient Descent(30/99): loss=0.13183935594859267\n",
      "Gradient Descent(40/99): loss=0.12479872393103913\n",
      "Gradient Descent(50/99): loss=0.11924027759198666\n",
      "Gradient Descent(60/99): loss=0.11482185247526101\n",
      "Gradient Descent(70/99): loss=0.11129022798612702\n",
      "Gradient Descent(80/99): loss=0.10845474755734597\n",
      "Gradient Descent(90/99): loss=0.10616992614771273\n",
      "Gradient Descent final loss = 0.10449102874174189\n",
      "Weights = [ 0.21724043  0.         -0.02499789  0.00726983 -0.07884486  0.02538187\n",
      "  0.0470352  -0.00154474 -0.00043374]\n"
     ]
    }
   ],
   "source": [
    "# Using Gradient-Descent regressor\n",
    "\n",
    "max_iters=100\n",
    "#With gamma = 0.7 loss huuuuge\n",
    "gamma=0.01\n",
    "\n",
    "w_initial=np.array(np.zeros(input_data.shape[1]))\n",
    "\n",
    "#Start\n",
    "GD_loss,GD_ws= gradient_descent(yb,input_data,w_initial,max_iters,gamma)\n",
    "\n",
    "#Print results\n",
    "print(\"Gradient Descent final loss =\",  GD_loss)\n",
    "print(\"Weights =\",GD_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  1.  1. ...  1.  1.  1.]\n",
      "Prediction file created\n"
     ]
    }
   ],
   "source": [
    "# Predicting the labels of test dataset.\n",
    "GD_labels=predict_labels(GD_ws,test_data)\n",
    "print(GD_labels)\n",
    "\n",
    "create_csv_submission(ids_test,GD_labels,'predictions_GD_PCA.csv')\n",
    "print('Prediction file created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Stochastic Gradient Descent\n",
    "\n",
    "#### Creation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic gradient Descent(0/99): loss=0.16903423424544467\n",
      "Stochastic gradient Descent(10/99): loss=0.15104776864779385\n",
      "Stochastic gradient Descent(20/99): loss=0.14081478451216126\n",
      "Stochastic gradient Descent(30/99): loss=0.13392797686636673\n",
      "Stochastic gradient Descent(40/99): loss=0.12569970255531177\n",
      "Stochastic gradient Descent(50/99): loss=0.1195343598577545\n",
      "Stochastic gradient Descent(60/99): loss=0.11530073981085984\n",
      "Stochastic gradient Descent(70/99): loss=0.11243724549607986\n",
      "Stochastic gradient Descent(80/99): loss=0.10940696642185664\n",
      "Stochastic gradient Descent(90/99): loss=0.10750270804489886\n",
      "Stochastic Gradient Descent final loss = 0.10567046452707281\n",
      "Weights = [ 0.2146644   0.         -0.02026381  0.01583881 -0.07634598  0.02046416\n",
      "  0.03428112  0.01635048 -0.01122279]\n"
     ]
    }
   ],
   "source": [
    "# Using Stochastic Gradient Descent regressor\n",
    "\n",
    "max_iters=100\n",
    "#With gamma = 0.7 loss huuuuge\n",
    "gamma=0.01\n",
    "batch_size=32\n",
    "\n",
    "w_initial=np.array(np.zeros(input_data.shape[1]))\n",
    "\n",
    "#Start\n",
    "SGD_loss,SGD_ws= stochastic_gradient_descent(yb,input_data,w_initial,batch_size,max_iters,gamma)\n",
    "\n",
    "#Print results\n",
    "print(\"Stochastic Gradient Descent final loss =\",  SGD_loss)\n",
    "print(\"Weights =\",SGD_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  1.  1. ...  1.  1.  1.]\n",
      "Prediction file created\n"
     ]
    }
   ],
   "source": [
    "# Predicting the labels of test dataset.\n",
    "SGD_labels=predict_labels(SGD_ws,test_data)\n",
    "print(SGD_labels)\n",
    "\n",
    "create_csv_submission(ids_test,SGD_labels,'predictions_SGD_PCA.csv')\n",
    "print('Prediction file created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../')\n",
    "from implementations import *\n",
    "\n",
    "# define parameter\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "degree=3\n",
    "    \n",
    "# form tx\n",
    "tx_tr = build_poly(input_data, degree)\n",
    "print(tx_tr.shape)\n",
    "\n",
    "# ridge regression with different lambda\n",
    "rmse_tr = []\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    # ridge regression\n",
    "    weight = ridge_regression(yb, tx_tr, lambda_)\n",
    "    rmse_tr.append(np.sqrt(2 * compute_loss_MSE(yb, tx_tr, weight)))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "\n",
    "plot_train_test(rmse_tr, rmse_te, lambdas, degree)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

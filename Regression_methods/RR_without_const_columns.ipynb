{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import time \n",
    "\n",
    "#to keep things in order, and to avoid to copy and paste everytime our functions if we want to use them in more than one folder,\n",
    "#we can temporarily use this library. \n",
    "import sys\n",
    "\n",
    "#in this way Python will search the implementations also in the path '../HelperFunctions'\n",
    "sys.path.insert(0, '../HelperFunctions')\n",
    "sys.path.insert(0, '../pre-processing/Clean_Data/')\n",
    "\n",
    "from proj1_helpers import *\n",
    "from common_functions import *\n",
    "from counters import *\n",
    "from remove import *\n",
    "from replace import *\n",
    "from regressors import *\n",
    "from CrossValidationFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeConstantColumns(data):\n",
    "    '''Remove columns which are constants from the data.\n",
    "       \n",
    "       Return data, idx_removed\n",
    "    '''\n",
    "    std = np.std(data, axis = 0)\n",
    "    idx_removed = np.where(std==0)[0]\n",
    "    if len(idx_removed >0 ):\n",
    "        data = np.delete(data,idx_removed,axis=1)\n",
    "    \n",
    "    return data, idx_removed\n",
    "\n",
    "def removeHighCorrelatedColumns(data, threshold = 0.8):\n",
    "    '''Remove columns which are highly correlated.\n",
    "       \n",
    "       WARNING: the returned list idx_removed MUST be used in a for loop on the test data, removing features one by one\n",
    "       \n",
    "       Return data, idx_removed\n",
    "    '''\n",
    "    #initialize idx_removed\n",
    "    idx_removed = []\n",
    "        \n",
    "    #Get first elements of the highly correlated couples\n",
    "    R = np.ma.corrcoef(data.T)\n",
    "    idx_HC = np.where( (R > threshold) & (R < 0.98))[0] \n",
    "\n",
    "    while(idx_HC.shape[0] > 0):\n",
    "        \n",
    "        idx_to_remove = idx_HC.max()\n",
    "        \n",
    "        data = np.delete(data, idx_to_remove, axis=1)\n",
    "        idx_removed.append(idx_to_remove)\n",
    "        \n",
    "        #compute the correlation coefficients of the reduced dataset\n",
    "        R = np.ma.corrcoef(data.T)\n",
    "        idx_HC = np.where( (R > threshold) & (R < 0.98))[0] \n",
    "        \n",
    "    \n",
    "    return data, idx_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data And Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(\"../data/train.csv\", sub_sample=False)\n",
    "_, test_data, ids_test = load_csv_data(\"../data/test.csv\", sub_sample=False)\n",
    "\n",
    "#this will surely be deleted, in this way we are sure that original_data is the original version of the data and we don't have\n",
    "#to load them again\n",
    "from copy import deepcopy\n",
    "originalData = deepcopy(input_data)\n",
    "originalY = deepcopy(yb)\n",
    "originalTest = deepcopy(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic step\n",
    "input_data = deepcopy(originalData)\n",
    "numInvalidValues=countInvalid(input_data, -999)\n",
    "idxCols = np.where(numInvalidValues>0)[0]\n",
    "input_data = replaceWithZero(input_data,-999,idxCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet division, removing constant columns, standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0, x1, x2\n",
    "idx0 = np.where(input_data[:,22]==0)\n",
    "idx1 = np.where(input_data[:,22]==1)\n",
    "idx2 = np.where(input_data[:,22]>=2)\n",
    "\n",
    "x0 = input_data[idx0] \n",
    "x1 = input_data[idx1] \n",
    "x2 = input_data[idx2] \n",
    "\n",
    "y0 = yb[idx0]\n",
    "y1 = yb[idx1]\n",
    "y2 = yb[idx2]\n",
    "\n",
    "x0, idx_constants_removed0 = removeConstantColumns(x0)\n",
    "x1, idx_constants_removed1 = removeConstantColumns(x1)\n",
    "\n",
    "x0, mean_train0, std_train0 = standardize ( x0 )\n",
    "x1, mean_train1, std_train1 = standardize ( x1 )\n",
    "x2, mean_train2, std_train2 = standardize ( x2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove HC columns\n",
    "\n",
    "Only if HC_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "HC_flag = False\n",
    "\n",
    "if(HC_flag):\n",
    "    threshold = 0.8\n",
    "\n",
    "    x0, idx_HC_removed0 = removeHighCorrelatedColumns(x0, threshold)\n",
    "    x1, idx_HC_removed1 = removeHighCorrelatedColumns(x1, threshold)\n",
    "    x2, idx_HC_removed2 = removeHighCorrelatedColumns(x2, threshold)\n",
    "else:\n",
    "    idx_HC_removed0 = []\n",
    "    idx_HC_removed0 = []\n",
    "    idx_HC_removed0 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CrossValidationFunctions import *\n",
    "def grid_search_hyperparam_with_CV(y, tx, lambdas, degrees):\n",
    "    loss_tr = np.zeros((len(lambdas), len(degrees)))\n",
    "    loss_te = np.zeros((len(lambdas), len(degrees)))\n",
    "    accuracy = np.zeros((len(lambdas), len(degrees)))\n",
    "    \n",
    "    for idx_lambda, lambda_ in enumerate(lambdas):\n",
    "        for idx_degree, degree in enumerate(degrees):\n",
    "                        \n",
    "            x_augmented = build_poly(tx, degree)\n",
    "            \n",
    "            #regression with your favourite method\n",
    "            k_indices = build_k_indices(y, 4, 1)\n",
    "            acc, loss1, loss2 = cross_validation_with_ridge(y, x_augmented, k_indices, lambda_)\n",
    "            \n",
    "            loss_tr[idx_lambda, idx_degree] = loss1\n",
    "            loss_te[idx_lambda, idx_degree] = loss2\n",
    "            accuracy[idx_lambda, idx_degree] = acc\n",
    "    \n",
    "    #find the best using the loss\n",
    "    min_loss_te = np.min(loss_te)\n",
    "    best_lambda_loss = lambdas[ np.where( loss_te == min_loss_te )[0] ]\n",
    "    best_degree_loss = degrees[ np.where( loss_te == min_loss_te )[1] ]\n",
    "\n",
    "    #recompute best w\n",
    "    x_augmented = build_poly(tx, int(best_degree_loss))\n",
    "    best_w_loss = ridge_regression(y,x_augmented,best_lambda_loss)\n",
    "    \n",
    "    #find the best using the accuracy\n",
    "    max_acc = np.max(accuracy)\n",
    "    best_lambda_acc = lambdas[ np.where( accuracy == max_acc )[0] ]\n",
    "    best_degree_acc = degrees[ np.where( accuracy == max_acc )[1] ]\n",
    "    \n",
    "    #recompute best w\n",
    "    x_augmented = build_poly(tx, int(best_degree_acc[0]))\n",
    "    best_w_acc = ridge_regression(y,x_augmented,best_lambda_acc[0])\n",
    "\n",
    "    return best_lambda_loss, best_degree_loss, best_w_loss, best_lambda_acc, best_degree_acc, best_w_acc, loss_tr, loss_te, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS\n",
      "Model with 0 jets: lambda = [0.00033667], degree = [10], loss = 34425736421631.133\n",
      "Model with 1 jets: lambda = [6.44444444e-05], degree = [10], loss = 41.33309562589343\n",
      "Model with more than 1 jets: lambda = [1.e-05], degree = [10], loss = 105.3151619605033\n",
      "\n",
      "\n",
      "ACCURACY\n",
      "Model with 0 jets: lambda = [1.e-05], degree = [11], acc = 0.8431920089678918\n",
      "Model with 1 jets: lambda = [0.00039111], degree = [12], acc = 0.8065614360878985\n",
      "Model with more than 1 jets: lambda = [0.00017333], degree = [13], acc = 0.8325613454645713\n",
      "\n",
      "\n",
      "Our test set reached an accuracy of: acc = 0.8287453635041816\n"
     ]
    }
   ],
   "source": [
    "lambdas0 = np.linspace(1e-5,0.0005,10) \n",
    "lambdas1 = np.linspace(1e-5,0.0005,10) \n",
    "lambdas2 = np.linspace(1e-5,0.0005,10) \n",
    "\n",
    "degrees = np.arange(10,15)\n",
    "\n",
    "\n",
    "best_lambda_loss0, best_degree_loss0, best_w_loss0, best_lambda_acc0, best_degree_acc0, best_w_acc0, loss_tr0, loss_te0, accuracy0 = \\\n",
    "grid_search_hyperparam_with_CV(y0, x0, lambdas0, degrees)\n",
    "\n",
    "best_lambda_loss1, best_degree_loss1, best_w_loss1, best_lambda_acc1, best_degree_acc1, best_w_acc1, loss_tr1, loss_te1, accuracy1 = \\\n",
    "grid_search_hyperparam_with_CV(y1, x1, lambdas1, degrees)\n",
    "\n",
    "best_lambda_loss2, best_degree_loss2, best_w_loss2, best_lambda_acc2, best_degree_acc2, best_w_acc2, loss_tr2, loss_te2, accuracy2 = \\\n",
    "grid_search_hyperparam_with_CV(y2, x2, lambdas2, degrees)\n",
    "\n",
    "print('LOSS')\n",
    "print(f'Model with 0 jets: lambda = {best_lambda_loss0}, degree = {best_degree_loss0}, loss = {np.min(loss_te0)}')\n",
    "print(f'Model with 1 jets: lambda = {best_lambda_loss1}, degree = {best_degree_loss1}, loss = {np.min(loss_te1)}')\n",
    "print(f'Model with more than 1 jets: lambda = {best_lambda_loss2}, degree = {best_degree_loss2}, loss = {np.min(loss_te2)}')\n",
    "\n",
    "print('\\n\\nACCURACY')\n",
    "print(f'Model with 0 jets: lambda = {best_lambda_acc0}, degree = {best_degree_acc0}, acc = {np.max(accuracy0)}')\n",
    "print(f'Model with 1 jets: lambda = {best_lambda_acc1}, degree = {best_degree_acc1}, acc = {np.max(accuracy1)}')\n",
    "print(f'Model with more than 1 jets: lambda = {best_lambda_acc2}, degree = {best_degree_acc2}, acc = {np.max(accuracy2)}')\n",
    "\n",
    "\n",
    "N0 = x0.shape[0]\n",
    "N1 = x1.shape[0]\n",
    "N2 = x2.shape[0]\n",
    "\n",
    "TOTAccuracy = ( N0*np.max(accuracy0) + N1*np.max(accuracy1) + N2*np.max(accuracy2) ) / ( N0 + N1 + N2 )\n",
    "print(f'\\n\\nOur test set reached an accuracy of: acc = {TOTAccuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS\n",
      "Model with 0 jets: lambda = [5.e-05], degree = [9], loss = 29265374737846.613\n",
      "Model with 1 jets: lambda = [6.44444444e-05], degree = [10], loss = 41.33309562589343\n",
      "Model with more than 1 jets: lambda = [1.e-05], degree = [10], loss = 105.3151619605033\n",
      "\n",
      "\n",
      "ACCURACY\n",
      "Model with 0 jets: lambda = [1.11888889e-05], degree = [11], acc = 0.8431719913523902\n",
      "Model with 1 jets: lambda = [0.00039111], degree = [12], acc = 0.8065614360878985\n",
      "Model with more than 1 jets: lambda = [0.00017333], degree = [13], acc = 0.8325613454645713\n",
      "\n",
      "\n",
      "Our test set reached an accuracy of: acc = 0.8287373634241112\n"
     ]
    }
   ],
   "source": [
    "#retrain only the first set\n",
    "\n",
    "lambdas0 = np.linspace(1e-7,5*1e-5,10) \n",
    "\n",
    "degrees = np.arange(9,13)\n",
    "\n",
    "\n",
    "best_lambda_loss0, best_degree_loss0, best_w_loss0, best_lambda_acc0, best_degree_acc0, best_w_acc0, loss_tr0, loss_te0, accuracy0 = \\\n",
    "grid_search_hyperparam_with_CV(y0, x0, lambdas0, degrees)\n",
    "\n",
    "print('LOSS')\n",
    "print(f'Model with 0 jets: lambda = {best_lambda_loss0}, degree = {best_degree_loss0}, loss = {np.min(loss_te0)}')\n",
    "print(f'Model with 1 jets: lambda = {best_lambda_loss1}, degree = {best_degree_loss1}, loss = {np.min(loss_te1)}')\n",
    "print(f'Model with more than 1 jets: lambda = {best_lambda_loss2}, degree = {best_degree_loss2}, loss = {np.min(loss_te2)}')\n",
    "\n",
    "print('\\n\\nACCURACY')\n",
    "print(f'Model with 0 jets: lambda = {best_lambda_acc0}, degree = {best_degree_acc0}, acc = {np.max(accuracy0)}')\n",
    "print(f'Model with 1 jets: lambda = {best_lambda_acc1}, degree = {best_degree_acc1}, acc = {np.max(accuracy1)}')\n",
    "print(f'Model with more than 1 jets: lambda = {best_lambda_acc2}, degree = {best_degree_acc2}, acc = {np.max(accuracy2)}')\n",
    "\n",
    "\n",
    "N0 = x0.shape[0]\n",
    "N1 = x1.shape[0]\n",
    "N2 = x2.shape[0]\n",
    "\n",
    "TOTAccuracy = ( N0*np.max(accuracy0) + N1*np.max(accuracy1) + N2*np.max(accuracy2) ) / ( N0 + N1 + N2 )\n",
    "print(f'\\n\\nOur test set reached an accuracy of: acc = {TOTAccuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission: import and basic steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = deepcopy(originalTest)\n",
    "num_tests = test_data.shape[0]\n",
    "\n",
    "numInvalidValues=countInvalid(test_data, -999)\n",
    "idxCols = np.where(numInvalidValues>0)[0]\n",
    "input_data = replaceWithZero(test_data,-999,idxCols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet division, removing constant/HC columns, standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0, x1, x2\n",
    "idx0 = np.where(test_data[:,22]==0)\n",
    "idx1 = np.where(test_data[:,22]==1)\n",
    "idx2 = np.where(test_data[:,22]>=2)\n",
    "\n",
    "x0 = test_data[idx0] \n",
    "x1 = test_data[idx1] \n",
    "x2 = test_data[idx2] \n",
    "\n",
    "x0 = np.delete(x0, idx_constants_removed0, axis=1)\n",
    "x1 = np.delete(x1, idx_constants_removed1, axis=1)\n",
    "\n",
    "x0,_,_ = standardize ( x0, mean_train0, std_train0 )\n",
    "x1,_,_ = standardize ( x1, mean_train1, std_train1 )\n",
    "x2,_,_ = standardize ( x2, mean_train2, std_train2 )\n",
    "\n",
    "if(HC_flag):\n",
    "    for i in idx_HC_removed0:\n",
    "        x0 = np.delete(x0,i,axis=1)\n",
    "    for i in idx_HC_removed1:\n",
    "        x0 = np.delete(x1,i,axis=1)\n",
    "    for i in idx_HC_removed2:\n",
    "        x0 = np.delete(x2,i,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat regression stuff and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = build_poly(x0, int(best_degree_acc0[0]))\n",
    "x1 = build_poly(x1, int(best_degree_acc1[0]))\n",
    "x2 = build_poly(x2, int(best_degree_acc2[0]))\n",
    "\n",
    "y_pred0 = predict_labels(best_w_acc0,x0)\n",
    "y_pred1 = predict_labels(best_w_acc1,x1)\n",
    "y_pred2 = predict_labels(best_w_acc2,x2)\n",
    "\n",
    "y_pred = np.ones(num_tests)\n",
    "y_pred[idx0] = y_pred0\n",
    "y_pred[idx1] = y_pred1\n",
    "y_pred[idx2] = y_pred2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, '04_RR_WithConstantRemoved.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment\n",
    "\n",
    "This is the prototype of our future regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import time \n",
    "\n",
    "#to keep things in order, and to avoid to copy and paste everytime our functions if we want to use them in more than one folder,\n",
    "#we can temporarily use this library. \n",
    "import sys\n",
    "\n",
    "#in this way Python will search the implementations also in the path '../HelperFunctions'\n",
    "sys.path.insert(0, '../HelperFunctions')\n",
    "sys.path.insert(0, '../pre-processing/Clean_Data/')\n",
    "\n",
    "from proj1_helpers import *\n",
    "from common_functions import *\n",
    "from counters import *\n",
    "from remove import *\n",
    "from replace import *\n",
    "from regressors import *\n",
    "from CrossValidationFunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data And Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(\"../data/train.csv\", sub_sample=False)\n",
    "_, test_data, ids_test = load_csv_data(\"../data/test.csv\", sub_sample=False)\n",
    "\n",
    "#this will surely be deleted, in this way we are sure that original_data is the original version of the data and we don't have\n",
    "#to load them again\n",
    "from copy import deepcopy\n",
    "originalData = deepcopy(input_data)\n",
    "originalY = deepcopy(yb)\n",
    "originalTest = deepcopy(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic step\n",
    "input_data = deepcopy(originalData)\n",
    "numInvalidValues=countInvalid(input_data, -999)\n",
    "idxCols = np.where(numInvalidValues>0)[0]\n",
    "input_data = replaceWithZero(input_data,-999,idxCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet division, removing constant columns, standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0, x1, x2\n",
    "idx0 = np.where(input_data[:,22]==0)\n",
    "idx1 = np.where(input_data[:,22]==1)\n",
    "idx2 = np.where(input_data[:,22]>=2)\n",
    "\n",
    "x0 = input_data[idx0] \n",
    "x1 = input_data[idx1] \n",
    "x2 = input_data[idx2] \n",
    "\n",
    "y0 = yb[idx0]\n",
    "y1 = yb[idx1]\n",
    "y2 = yb[idx2]\n",
    "\n",
    "x0, idx_constants_removed0 = removeConstantColumns(x0)\n",
    "x1, idx_constants_removed1 = removeConstantColumns(x1)\n",
    "\n",
    "x0, mean_train0, std_train0 = standardize ( x0 )\n",
    "x1, mean_train1, std_train1 = standardize ( x1 )\n",
    "x2, mean_train2, std_train2 = standardize ( x2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove HC columns\n",
    "\n",
    "Only if HC_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HC_flag = False\n",
    "\n",
    "if(HC_flag):\n",
    "    threshold = 0.8\n",
    "\n",
    "    x0, idx_HC_removed0 = removeHighCorrelatedColumns(x0, threshold)\n",
    "    x1, idx_HC_removed1 = removeHighCorrelatedColumns(x1, threshold)\n",
    "    x2, idx_HC_removed2 = removeHighCorrelatedColumns(x2, threshold)\n",
    "else:\n",
    "    idx_HC_removed0 = []\n",
    "    idx_HC_removed0 = []\n",
    "    idx_HC_removed0 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambdas0 = np.linspace(0.0001,0.01,15) #for the first subset the lambda is around 0.001\n",
    "# lambdas1 = np.linspace(0.00001,0.001,15) #for the second subset the lambda is around 0.0001\n",
    "# lambdas2 = np.linspace(0.00001,0.001,15) #for the third subset the lambda is around 0.0001\n",
    "\n",
    "# degrees = np.arange(7,17) #the best degree was high for all the models\n",
    "\n",
    "\n",
    "# best_lambda_loss0, best_degree_loss0, best_w_loss0, best_lambda_acc0, best_degree_acc0, best_w_acc0, loss_tr0, loss_te0, accuracy0 = \\\n",
    "# grid_search_hyperparam_with_CV(y0, x0, lambdas0, degrees)\n",
    "\n",
    "# best_lambda_loss1, best_degree_loss1, best_w_loss1, best_lambda_acc1, best_degree_acc1, best_w_acc1, loss_tr1, loss_te1, accuracy1 = \\\n",
    "# grid_search_hyperparam_with_CV(y1, x1, lambdas1, degrees)\n",
    "\n",
    "# best_lambda_loss2, best_degree_loss2, best_w_loss2, best_lambda_acc2, best_degree_acc2, best_w_acc2, loss_tr2, loss_te2, accuracy2 = \\\n",
    "# grid_search_hyperparam_with_CV(y2, x2, lambdas2, degrees)\n",
    "\n",
    "# print('LOSS')\n",
    "# print(f'Model with 0 jets: lambda = {best_lambda_loss0}, degree = {best_degree_loss0}, loss = {np.min(loss_te0)}')\n",
    "# print(f'Model with 1 jets: lambda = {best_lambda_loss1}, degree = {best_degree_loss1}, loss = {np.min(loss_te1)}')\n",
    "# print(f'Model with more than 1 jets: lambda = {best_lambda_loss2}, degree = {best_degree_loss2}, loss = {np.min(loss_te2)}')\n",
    "\n",
    "# print('\\n\\nACCURACY')\n",
    "# print(f'Model with 0 jets: lambda = {best_lambda_acc0}, degree = {best_degree_acc0}, acc = {np.max(accuracy0)}')\n",
    "# print(f'Model with 1 jets: lambda = {best_lambda_acc1}, degree = {best_degree_acc1}, acc = {np.max(accuracy1)}')\n",
    "# print(f'Model with more than 1 jets: lambda = {best_lambda_acc2}, degree = {best_degree_acc2}, acc = {np.max(accuracy2)}')\n",
    "\n",
    "\n",
    "# N0 = x0.shape[0]\n",
    "# N1 = x1.shape[0]\n",
    "# N2 = x2.shape[0]\n",
    "\n",
    "# TOTAccuracy = ( N0*np.max(accuracy0) + N1*np.max(accuracy1) + N2*np.max(accuracy2) ) / ( N0 + N1 + N2 )\n",
    "# print(f'\\n\\nOur test set reached an accuracy of: acc = {TOTAccuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission: import and basic steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = deepcopy(originalTest)\n",
    "num_tests = test_data.shape[0]\n",
    "\n",
    "numInvalidValues=countInvalid(test_data, -999)\n",
    "idxCols = np.where(numInvalidValues>0)[0]\n",
    "input_data = replaceWithZero(test_data,-999,idxCols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet division, removing constant/HC columns, standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0, x1, x2\n",
    "idx0 = np.where(test_data[:,22]==0)\n",
    "idx1 = np.where(test_data[:,22]==1)\n",
    "idx2 = np.where(test_data[:,22]>=2)\n",
    "\n",
    "x0 = test_data[idx0] \n",
    "x1 = test_data[idx1] \n",
    "x2 = test_data[idx2] \n",
    "\n",
    "x0 = np.delete(x0, idx_constants_removed0, axis=1)\n",
    "x1 = np.delete(x1, idx_constants_removed1, axis=1)\n",
    "\n",
    "x0,_,_ = standardizeWithGivenParameters ( x0, mean_train0, std_train0 )\n",
    "x1,_,_ = standardizeWithGivenParameters ( x1, mean_train1, std_train1 )\n",
    "x2,_,_ = standardizeWithGivenParameters ( x2, mean_train2, std_train2 )\n",
    "\n",
    "if(HC_flag):\n",
    "    for i in idx_HC_removed0:\n",
    "        x0 = np.delete(x0,i,axis=1)\n",
    "    for i in idx_HC_removed1:\n",
    "        x1 = np.delete(x1,i,axis=1)\n",
    "    for i in idx_HC_removed2:\n",
    "        x2 = np.delete(x2,i,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat regression stuff and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = build_poly(x0, int(best_degree_acc0[0]))\n",
    "x1 = build_poly(x1, int(best_degree_acc1[0]))\n",
    "x2 = build_poly(x2, int(best_degree_acc2[0]))\n",
    "\n",
    "y_pred0 = predict_labels(best_w_acc0,x0)\n",
    "y_pred1 = predict_labels(best_w_acc1,x1)\n",
    "y_pred2 = predict_labels(best_w_acc2,x2)\n",
    "\n",
    "y_pred = np.ones(num_tests)\n",
    "y_pred[idx0] = y_pred0\n",
    "y_pred[idx1] = y_pred1\n",
    "y_pred[idx2] = y_pred2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, 'dummy_name.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

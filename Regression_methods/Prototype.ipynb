{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment\n",
    "\n",
    "This is the prototype of our future regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import time \n",
    "\n",
    "#to keep things in order, and to avoid to copy and paste everytime our functions if we want to use them in more than one folder,\n",
    "#we can temporarily use this library. \n",
    "import sys\n",
    "\n",
    "#in this way Python will search the implementations also in the path '../HelperFunctions'\n",
    "sys.path.insert(0, '../HelperFunctions')\n",
    "sys.path.insert(0, '../pre-processing/Clean_Data/')\n",
    "sys.path.insert(0, '../Logit')\n",
    "\n",
    "from proj1_helpers import *\n",
    "from common_functions import *\n",
    "from counters import *\n",
    "from remove import *\n",
    "from replace import *\n",
    "from regressors import *\n",
    "from CrossValidationFunctions import *\n",
    "from functions_logistic import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_logistic_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred=sigmoid(y_pred)\n",
    "    y_pred[np.where(y_pred <= 0.5)] = 0\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "    \n",
    "    return y_pred\n",
    "            \n",
    "def convert_0_to_minus1(data):\n",
    "    data[data == 0]= -1\n",
    "    return data\n",
    "\n",
    "def convert_minus1_to_0(data):\n",
    "    data[data == -1]= 0\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss and gradient.\"\"\"\n",
    "    #num_samples = y.shape[0]\n",
    "    loss = calculate_logistic_loss(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    gradient = calculate_logistic_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "    return loss, gradient\n",
    "\n",
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w -= gamma * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_hyperparam_with_CV(y, tx, lambdas, gamma, degrees, max_iter):\n",
    "\n",
    "    accuracy = np.zeros((len(lambdas), len(degrees)))\n",
    "    \n",
    "    for idx_lambda, lambda_ in enumerate(lambdas):\n",
    "        for idx_degree, degree in enumerate(degrees):\n",
    "                        \n",
    "            x_augmented = build_poly(tx, degree)\n",
    "            initial_w = np.ones((x_augmented.shape[1]))\n",
    "            \n",
    "            #regression with logistic method\n",
    "            k_indices = build_k_indices(y, 4, 1)\n",
    "            acc = cross_validation_with_logistic(y, x_augmented, k_indices, initial_w, gamma, lambda_, max_iter)        \n",
    "            accuracy[idx_lambda, idx_degree] = acc\n",
    "    \n",
    "    #find the best using the accuracy\n",
    "    max_acc = np.max(accuracy)\n",
    "    print('max acc = ',max_acc)\n",
    "    coordinates_best_parameter = np.where( accuracy == max_acc )\n",
    "    print('coordinates_best_parameter: ', coordinates_best_parameter)\n",
    "    best_lambda_acc = lambdas[ coordinates_best_parameter[0][0] ]\n",
    "    best_degree_acc = degrees[ coordinates_best_parameter[1][0] ]\n",
    "\n",
    "    return best_lambda_acc, best_degree_acc, max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_with_logistic(y, x, k_indices,initial_w, gamma, lambda_,max_iter):\n",
    "    \"\"\"CV regression according to the splitting in train/test given by k_indices.\n",
    "    \n",
    "    The returned quantities are the average of the quantities computed in the single folds\n",
    "    \n",
    "    return the accuracy\"\"\"\n",
    "    \n",
    "    folds = k_indices.shape[0]\n",
    "    accuracy = np.zeros(folds)\n",
    "    w=initial_w\n",
    "    \n",
    "    for k in range(folds):\n",
    "        \n",
    "        #split the data in train/test\n",
    "        idx = k_indices[k]\n",
    "        yte = y[idx]\n",
    "        if len( x.shape ) == 1:\n",
    "            xte = x[idx]\n",
    "        else:\n",
    "            xte = x[idx,:]\n",
    "            \n",
    "        ytr = np.delete(y,idx,0)\n",
    "        xtr = np.delete(x,idx,0)\n",
    "\n",
    "        #learning by penalized graient descent (with regularized logistic)\n",
    "        for iter_ in range(max_iter):\n",
    "            _, w = learning_by_penalized_gradient(ytr, xtr, w, gamma, lambda_)\n",
    "            \n",
    "        #accuracy\n",
    "        y_pred = predict_logistic_labels(w, xte)\n",
    "        accuracy[k] = np.sum(y_pred == yte) / len(yte) \n",
    "        \n",
    "        if k == 2:\n",
    "            print('second K fold in CV')\n",
    "   \n",
    "    return np.mean(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeConstantColumns(data):\n",
    "    '''Remove columns which are constants from the data.\n",
    "       \n",
    "       Return data, idx_removed\n",
    "    '''\n",
    "    std = np.std(data, axis = 0)\n",
    "    idx_removed = np.where(std==0)[0]\n",
    "    if len(idx_removed >0 ):\n",
    "        data = np.delete(data,idx_removed,axis=1)\n",
    "    \n",
    "    return data, idx_removed\n",
    "\n",
    "def removeHighCorrelatedColumns(data, threshold = 0.8):\n",
    "    '''Remove columns which are highly correlated.\n",
    "       \n",
    "       WARNING: the returned list idx_removed MUST be used in a for loop on the test data, removing features one by one\n",
    "       \n",
    "       Return data, idx_removed\n",
    "    '''\n",
    "    #initialize idx_removed\n",
    "    idx_removed = []\n",
    "        \n",
    "    #Get first elements of the highly correlated couples\n",
    "    R = np.ma.corrcoef(data.T)\n",
    "    idx_HC = np.where( (R > threshold) & (R < 0.98))[0] \n",
    "\n",
    "    while(idx_HC.shape[0] > 0):\n",
    "        \n",
    "        idx_to_remove = idx_HC.max()\n",
    "        \n",
    "        data = np.delete(data, idx_to_remove, axis=1)\n",
    "        idx_removed.append(idx_to_remove)\n",
    "        \n",
    "        #compute the correlation coefficients of the reduced dataset\n",
    "        R = np.ma.corrcoef(data.T)\n",
    "        idx_HC = np.where( (R > threshold) & (R < 0.98))[0] \n",
    "        \n",
    "    \n",
    "    return data, idx_removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data And Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(\"../data/train.csv\", sub_sample=False)\n",
    "_, test_data, ids_test = load_csv_data(\"../data/test.csv\", sub_sample=False)\n",
    "\n",
    "#this will surely be deleted, in this way we are sure that original_data is the original version of the data and we don't have\n",
    "#to load them again\n",
    "from copy import deepcopy\n",
    "originalData = deepcopy(input_data)\n",
    "originalY = deepcopy(yb)\n",
    "originalTest = deepcopy(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic step\n",
    "input_data = deepcopy(originalData)\n",
    "input_data = replaceWithZero(input_data,-999,idxCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet division, removing constant columns, standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0, x1, x2\n",
    "idx0 = np.where(input_data[:,22]==0)\n",
    "idx1 = np.where(input_data[:,22]==1)\n",
    "idx2 = np.where(input_data[:,22]>=2)\n",
    "\n",
    "x0 = input_data[idx0] \n",
    "x1 = input_data[idx1] \n",
    "x2 = input_data[idx2] \n",
    "\n",
    "y0 = yb[idx0]\n",
    "y1 = yb[idx1]\n",
    "y2 = yb[idx2]\n",
    "\n",
    "x0, idx_constants_removed0 = removeConstantColumns(x0)\n",
    "x1, idx_constants_removed1 = removeConstantColumns(x1)\n",
    "\n",
    "x0, mean_train0, std_train0 = standardize ( x0 )\n",
    "x1, mean_train1, std_train1 = standardize ( x1 )\n",
    "x2, mean_train2, std_train2 = standardize ( x2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = convert_minus1_to_0(y0)\n",
    "y1 = convert_minus1_to_0(y1)\n",
    "y2 = convert_minus1_to_0(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove HC columns\n",
    "\n",
    "Only if HC_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "HC_flag = True\n",
    "\n",
    "if(HC_flag):\n",
    "    threshold = 0.8\n",
    "\n",
    "    x0, idx_HC_removed0 = removeHighCorrelatedColumns(x0, threshold)\n",
    "    x1, idx_HC_removed1 = removeHighCorrelatedColumns(x1, threshold)\n",
    "    x2, idx_HC_removed2 = removeHighCorrelatedColumns(x2, threshold)\n",
    "else:\n",
    "    idx_HC_removed0 = []\n",
    "    idx_HC_removed0 = []\n",
    "    idx_HC_removed0 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iter=500\n",
    "gamma=1e-5\n",
    "\n",
    "w_initial_x0 = np.ones((x0.shape[1]))\n",
    "w_initial_x1 = np.ones((x1.shape[1]))\n",
    "w_initial_x2 = np.ones((x2.shape[1]))\n",
    "\n",
    "for i in range(max_iter):\n",
    "    _, w_x0 = learning_by_gradient_descent(y0, x0, w_initial_x0, gamma)\n",
    "    _, w_x1 = learning_by_gradient_descent(y1, x1, w_initial_x1, gamma)\n",
    "    _, w_x2 = learning_by_gradient_descent(y2, x2, w_initial_x2, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambdas0 = np.linspace(0.0001,0.01,15) #for the first subset the lambda is around 0.001\n",
    "# lambdas1 = np.linspace(0.00001,0.001,15) #for the second subset the lambda is around 0.0001\n",
    "# lambdas2 = np.linspace(0.00001,0.001,15) #for the third subset the lambda is around 0.0001\n",
    "\n",
    "# degrees = np.arange(7,17) #the best degree was high for all the models\n",
    "\n",
    "\n",
    "# best_lambda_loss0, best_degree_loss0, best_w_loss0, best_lambda_acc0, best_degree_acc0, best_w_acc0, loss_tr0, loss_te0, accuracy0 = \\\n",
    "# grid_search_hyperparam_with_CV(y0, x0, lambdas0, degrees)\n",
    "\n",
    "# best_lambda_loss1, best_degree_loss1, best_w_loss1, best_lambda_acc1, best_degree_acc1, best_w_acc1, loss_tr1, loss_te1, accuracy1 = \\\n",
    "# grid_search_hyperparam_with_CV(y1, x1, lambdas1, degrees)\n",
    "\n",
    "# best_lambda_loss2, best_degree_loss2, best_w_loss2, best_lambda_acc2, best_degree_acc2, best_w_acc2, loss_tr2, loss_te2, accuracy2 = \\\n",
    "# grid_search_hyperparam_with_CV(y2, x2, lambdas2, degrees)\n",
    "\n",
    "# print('LOSS')\n",
    "# print(f'Model with 0 jets: lambda = {best_lambda_loss0}, degree = {best_degree_loss0}, loss = {np.min(loss_te0)}')\n",
    "# print(f'Model with 1 jets: lambda = {best_lambda_loss1}, degree = {best_degree_loss1}, loss = {np.min(loss_te1)}')\n",
    "# print(f'Model with more than 1 jets: lambda = {best_lambda_loss2}, degree = {best_degree_loss2}, loss = {np.min(loss_te2)}')\n",
    "\n",
    "# print('\\n\\nACCURACY')\n",
    "# print(f'Model with 0 jets: lambda = {best_lambda_acc0}, degree = {best_degree_acc0}, acc = {np.max(accuracy0)}')\n",
    "# print(f'Model with 1 jets: lambda = {best_lambda_acc1}, degree = {best_degree_acc1}, acc = {np.max(accuracy1)}')\n",
    "# print(f'Model with more than 1 jets: lambda = {best_lambda_acc2}, degree = {best_degree_acc2}, acc = {np.max(accuracy2)}')\n",
    "\n",
    "\n",
    "# N0 = x0.shape[0]\n",
    "# N1 = x1.shape[0]\n",
    "# N2 = x2.shape[0]\n",
    "\n",
    "# TOTAccuracy = ( N0*np.max(accuracy0) + N1*np.max(accuracy1) + N2*np.max(accuracy2) ) / ( N0 + N1 + N2 )\n",
    "# print(f'\\n\\nOur test set reached an accuracy of: acc = {TOTAccuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only for regLogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only for regularized Logit\n",
    "gamma=1e-5\n",
    "\n",
    "x0_augmented = build_poly(x0, best_degree_acc_0)\n",
    "x1_augmented = build_poly(x1, best_degree_acc_1)\n",
    "x2_augmented = build_poly(x2, best_degree_acc_2)\n",
    "\n",
    "initial_w_x0 = np.ones((x0_augmented.shape[1]))\n",
    "initial_w_x1 = np.ones((x1_augmented.shape[1]))\n",
    "initial_w_x2 = np.ones((x2_augmented.shape[1]))\n",
    "\n",
    "_, best_w_0 = learning_by_penalized_gradient(y0, x0_augmented, initial_w_x0, gamma, best_lambda_acc_0)\n",
    "_, best_w_1 = learning_by_penalized_gradient(y1, x1_augmented, initial_w_x1, gamma, best_lambda_acc_1)\n",
    "_, best_w_2 = learning_by_penalized_gradient(y2, x2_augmented, initial_w_x2, gamma, best_lambda_acc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission: import and basic steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = deepcopy(originalTest)\n",
    "num_tests = test_data.shape[0]\n",
    "\n",
    "numInvalidValues=countInvalid(test_data, -999)\n",
    "idxCols = np.where(numInvalidValues>0)[0]\n",
    "input_data = replaceWithZero(test_data,-999,idxCols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jet division, removing constant/HC columns, standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0, x1, x2\n",
    "idx0 = np.where(test_data[:,22]==0)\n",
    "idx1 = np.where(test_data[:,22]==1)\n",
    "idx2 = np.where(test_data[:,22]>=2)\n",
    "\n",
    "test_x0 = test_data[idx0] \n",
    "test_x1 = test_data[idx1] \n",
    "test_x2 = test_data[idx2] \n",
    "\n",
    "test_x0 = np.delete(test_x0, idx_constants_removed0, axis=1)\n",
    "test_x1 = np.delete(test_x1, idx_constants_removed1, axis=1)\n",
    "\n",
    "test_x0,_,_ = standardize ( test_x0, mean_train0, std_train0 )\n",
    "test_x1,_,_ = standardize ( test_x1, mean_train1, std_train1 )\n",
    "test_x2,_,_ = standardize ( test_x2, mean_train2, std_train2 )\n",
    "\n",
    "if(HC_flag):\n",
    "    for i in idx_HC_removed0:\n",
    "        test_x0 = np.delete(test_x0,i,axis=1)\n",
    "    for i in idx_HC_removed1:\n",
    "        test_x1 = np.delete(test_x1,i,axis=1)\n",
    "    for i in idx_HC_removed2:\n",
    "        test_x2 = np.delete(test_x2,i,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat regression stuff and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only for regLogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retraining on the whole dataset using best hyperparameters\n",
    "gamma = 1e-5\n",
    "max_iter = 100\n",
    "\n",
    "x0_augmented = build_poly(x0, best_degree_acc_0)\n",
    "x1_augmented = build_poly(x1, best_degree_acc_1)\n",
    "x2_augmented = build_poly(x2, best_degree_acc_2)\n",
    "\n",
    "initial_w_x0 = np.ones((x0_augmented.shape[1]))\n",
    "initial_w_x1 = np.ones((x1_augmented.shape[1]))\n",
    "initial_w_x2 = np.ones((x2_augmented.shape[1]))\n",
    "\n",
    "for iter_ in range(max_iter):\n",
    "\n",
    "    _, best_w_0 = learning_by_penalized_gradient(y0, x0_augmented, initial_w_x0, gamma, best_lambda_acc_0)\n",
    "    _, best_w_1 = learning_by_penalized_gradient(y1, x1_augmented, initial_w_x1, gamma, best_lambda_acc_1)\n",
    "    _, best_w_2 = learning_by_penalized_gradient(y2, x2_augmented, initial_w_x2, gamma, best_lambda_acc_2)\n",
    "\n",
    "#design of the test set for our model\n",
    "test_x0_augmented = build_poly(test_x0,best_degree_acc_0)\n",
    "test_x1_augmented = build_poly(test_x1,best_degree_acc_1)\n",
    "test_x2_augmented = build_poly(test_x2,best_degree_acc_2)\n",
    "\n",
    "\n",
    "y_pred0 = predict_logistic_labels(best_w_0,test_x0_augmented)\n",
    "y_pred1 = predict_logistic_labels(best_w_1,test_x1_augmented)\n",
    "y_pred2 = predict_logistic_labels(best_w_2,test_x2_augmented)\n",
    "\n",
    "\n",
    "y_pred = np.ones(num_tests)\n",
    "y_pred[idx0] = y_pred0\n",
    "y_pred[idx1] = y_pred1\n",
    "y_pred[idx2] = y_pred2\n",
    "\n",
    "y_pred = convert_0_to_minus1(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prediction for Logit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred0 = predict_logistic_labels(w_x0,test_x0)\n",
    "y_pred1 = predict_logistic_labels(w_x1,test_x1)\n",
    "y_pred2 = predict_logistic_labels(w_x2,test_x2)\n",
    "\n",
    "\n",
    "y_pred = np.ones(num_tests)\n",
    "y_pred[idx0] = y_pred0\n",
    "y_pred[idx1] = y_pred1\n",
    "y_pred[idx2] = y_pred2\n",
    "\n",
    "y_pred = convert_0_to_minus1(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, 'Logit_RemovConstColumns_RemovHCcolumns_Normal_WithZero_JetDiv_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
